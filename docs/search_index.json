[["index.html", "DLL 2021, R section 1 Workshop Introduction", " DLL 2021, R section mesako Margaret Samson Zac darachm Typeset on 2023-01-03 1 Workshop Introduction This short 3-day course in R aims to give you a basic framework and skills for working effectively with your research mentors. Together, we will get oriented with basic skills (e.g. using RStudio, documenting your process with R Markdown, reading data in, basic data analysis, and visualization) and concepts for how to organize your research workflows in R. We intend for this starting point to empower you to accomplish research-related tasks in R. R is a high-level data analysis scripting language 1. While it is very easy to write programs in this language, it is designed first as an environment that stitches together cutting edge research methods with flexible visualization and reporting frameworks. R has swept to be the de facto high-level language for data analysis because of the rich ecosystem of dispersed open-source developers. Here’s some examples of plots you generate in R. Here’s an example of the types of workflows and analyses you can generate in R (all the plots, and the website too). Heck, this website is generated by the R package bookdown from Rmd files, which you will learn to write. and also a “GNU” project, apparently!↩︎ "],["workshop-goals.html", "1.1 Workshop goals", " 1.1 Workshop goals We aim for all participants to be able to: use the Rstudio IDE (open source edition) know how to store and manipulate data in variables read in data from computer files in various formats process these with functions to generate statistical summaries turn these into various plots using the base graphics and ggplot2 library read in packages from various sources and know how to start using them do these steps in workflows that scale to analyzing many many files write all of this up as an Rmarkdown file to report your analysis and findings to collaborators "],["structure-and-resources.html", "1.2 Structure and resources", " 1.2 Structure and resources 1.2.1 Workshop schedule Each day will have a slightly different schedule, but you can expect a mix of synchronous and asynchronous work sessions, as well as two breaks in that day’s workshop. Please visit each day’s specific page for the exact schedule: Day 3 Schedule Day 4 Schedule Day 5 Schedule Each time segment lists who among the teaching team will be around to assist. We encourage you to poke the Slack channel (so below for info) with questions, but you can feel free to tag (@) or directly message the person responsible for the time slot/content you have questions about. 1.2.2 Asynchronous sessions Asynchronous here means self-paced learning that takes place off-Zoom. You will be expected to progress through this website during asynchronous work time in this 3-day period. We developed this website/document for your reference, as a living textbook and collection of “slides” and code snippets. As you go through this website during the asynchronous sessions, you should also complete the exercises provided in the accompanying worksheets. If you have questions and/or need help, you should reach out to us and your peers on Slack. We are also happy to jump into a Zoom call to work through issues with you. Tips: You can shrink the table of contents (left) by clicking the four lines icon in the top menu. You can click on footnotes 2. 1.2.2.1 Slack channel While you are working in asynchronous sessions, or if you just need help during the remainder of your program, there is a Slack channel available where you can go for ideas/help. The channel is called #learn-R and should be accessible to you on the SSRP Slack server. 1.2.3 Synchronous sessions Synchronous here means live, group learning that takes place on-Zoom. During synchronous meetings, you should plan to work directly with your peers and us on focused tasks. We will also be there to help with confusing or challenging topics that you want to discuss with someone live. During the schedule synchronous session (check each day’s schedule for timing), you should log on to the provided Zoom link. to read them, then go back by clicking the arrow↩︎ "],["workshop-expectations.html", "1.3 Workshop expectations", " 1.3 Workshop expectations Be respectful and compassionate. Teach one another, learn from one another. Aim for productive struggle. You will learn best if you make a good faith effort before seeking help. However, you should always seek help if you feel truly stuck. Create your own sense of challenge. Pick activities that you will learn and grow from. If you don’t find something challenging, make it challenging for yourself. "],["day-4---tidyverse-and-visualizations.html", "2 Day 4 - tidyverse and visualizations", " 2 Day 4 - tidyverse and visualizations Learning Goals: By the end of today’s session, students will be able to: Read in and process data starting from a local saved file. Transform, scale, filter, and convert values within a dataset. Manipulate data frames using tidyverse piping and functions. Reshape a data frame so that it is “tidy” and appropriate for a given analysis. Describe the roles of data, aesthetics, and geoms in ggplot functions. Choose the correct aesthetics and alter the geom parameters for a scatter plot, histogram, or box plot. Customize plot scales, titles, subtitles, themes, fonts, layout, and orientation. Layer multiple geoms in a single plot, including error bars for data analysis. Save a ggplot to a local file. Analyze dataset using a variety of statistical approaches. Preparing for Today: Please make sure you have completed all readings and activities from Section 2: Day 3 - introduction to R. You will also have some time at the start of the day to do the first sections 3.1-3.2. Before you start reading through Section 3 for today or taking part in synchronous activities, download and open today’s worksheet. On this Github page, please right-click the button that says “Raw” and click “Save Link As…” to get a local copy of this Rmd file on your computer and open it in Rstudio. Today’s Schedule: Start Time End Time Activity Description Facilitator 10:00 AM PDT 10:45 AM PDT Asynchronous Work through sections 3.1-3.2 Margaret, Darach 10:45 AM PDT 11:30 AM PDT Synchronous Peer coding exercises on Zoom Margaret, Melissa, Samson, Zac, Darach 11:30 AM PDT 12:15 PM PDT Asynchronous Work through sections 3.4-3.5, 3.4.5 is optional Melissa, Margaret, Darach 12:15 PM PDT 12:45 PM PDT Lunch Break Take a computer break! N/A 12:45 PM PDT 1:30 PM PDT Synchronous Peer coding exercises on Zoom Melissa, Margaret, Zac 1:30 PM PDT 2:00 PM PDT Asynchronous Work through sections 3.6 Margaret, Samson, Darach 2:00 PM PDT 2:30 PM PDT Snack Break Take a computer break! N/A 2:30 PM PDT 3:30 PM PDT Synchronous Wrap-up session on Zoom Samson, Margaret, Zac "],["reading-and-processing-data.html", "2.1 Reading and processing data", " 2.1 Reading and processing data So far, we have largely used data that was already provided to us in base R or through the tidyverse packages. In reality, you will be working with data that comes from a file containing measurements collected by a researcher. 2.1.1 Loading a file into R There are multiple file types that can be read into R. The most common one is the csv file extension, though txt files also work. You can read in Excel files using additional packages such as the readxl library. Let’s practice reading in data using the read.csv function. ?read.csv The repository for this site has a folder called data that contains a file called iris.csv, which is simply the original iris dataset with some modifications. We read in this data and save it to a variable named irisz (worksheet task 3.1.1A). irisz &lt;- read.csv(&quot;data/iris.csv&quot;) head(irisz) ## Sepal.Length Sepal.Width Petal.Length Petal.Width Species Location ## 1 5.1 3.5 1.4 0.2 setosa Korea ## 2 4.9 3 1.4 0.2 setosa China ## 3 4.7 3.2 1.3 0.2 setosa Korea ## 4 4.6 3.1 1.5 0.2 setosa China ## 5 5 3.6 1.4 0.2 setosa China ## 6 5.4 1.7 0.4 setosa Canada You can already see there are some differences compared to the original iris dataset, such as missing data and an additional column of information. You can see there is also a difference in how certain columns are handled. head(iris$Species) ## [1] setosa setosa setosa setosa setosa setosa ## Levels: setosa versicolor virginica head(irisz$Species) ## [1] &quot;setosa&quot; &quot;setosa&quot; &quot;setosa&quot; &quot;setosa&quot; &quot;setosa&quot; &quot;setosa&quot; For example, here it is clear that our data we read from a csv file is not treating species as categorical information. We can toggle a setting called stringsAsFactors that will turn these character string inputs into factors when the file is loaded into R. irisz &lt;- read.csv(&quot;data/iris.csv&quot;, stringsAsFactors = TRUE) head(irisz$Species) ## [1] setosa setosa setosa setosa setosa setosa ## Levels: setosa versicolor virginica head(irisz$Location) ## [1] Korea China Korea China China Canada ## Levels: Canada China Japan Korea Russia USA If we did not want to reload our data from a file, we can also transform columns (i.e. vectors) in our dataframe to be different types. For example, we could use irisz$Species &lt;- as.factor(irisz$Species) to turn it into a factor after it has already been loaded. There are similar functions like as.numeric, as.character, and as.logical that you may use to transform data types. Be aware though that certain conversions may lead to issues and loss of data! Now both of these columns are being treated as categorical data. There are other settings we can change, such as providing different names for the columns (worksheet task 3.1.1B). irisz &lt;- read.csv(&quot;data/iris.csv&quot;, col.names = c(&quot;sep_len&quot;, &quot;sep_wid&quot;, &quot;pet_len&quot;, &quot;pet_wid&quot;, &quot;species&quot;, &quot;loc&quot;)) head(irisz) ## sep_len sep_wid pet_len pet_wid species loc ## 1 5.1 3.5 1.4 0.2 setosa Korea ## 2 4.9 3 1.4 0.2 setosa China ## 3 4.7 3.2 1.3 0.2 setosa Korea ## 4 4.6 3.1 1.5 0.2 setosa China ## 5 5 3.6 1.4 0.2 setosa China ## 6 5.4 1.7 0.4 setosa Canada 2.1.2 Handling issues in data Now that we have loaded in the data, we should preview it and look for any possible issues. You never know what can go wrong in the data pipeline: data can be recorded incorrectly or corrupted at any point in the process. str(irisz) ## &#39;data.frame&#39;: 150 obs. of 6 variables: ## $ sep_len: chr &quot;5.1&quot; &quot;4.9&quot; &quot;4.7&quot; &quot;4.6&quot; ... ## $ sep_wid: chr &quot;3.5&quot; &quot;3&quot; &quot;3.2&quot; &quot;3.1&quot; ... ## $ pet_len: chr &quot;1.4&quot; &quot;1.4&quot; &quot;1.3&quot; &quot;1.5&quot; ... ## $ pet_wid: chr &quot;0.2&quot; &quot;0.2&quot; &quot;0.2&quot; &quot;0.2&quot; ... ## $ species: chr &quot;setosa&quot; &quot;setosa&quot; &quot;setosa&quot; &quot;setosa&quot; ... ## $ loc : chr &quot;Korea&quot; &quot;China&quot; &quot;Korea&quot; &quot;China&quot; ... summary(irisz) ## sep_len sep_wid pet_len pet_wid ## Length:150 Length:150 Length:150 Length:150 ## Class :character Class :character Class :character Class :character ## Mode :character Mode :character Mode :character Mode :character ## species loc ## Length:150 Length:150 ## Class :character Class :character ## Mode :character Mode :character One immediate issue you can spot here is that all variables are being treated as character strings. We would expect that sepal length/width and petal length/width would be numeric variables. To inspect what might be happening, we can look at the unique elements in one of these columns using the unique function. unique(irisz$sep_len) ## [1] &quot;5.1&quot; &quot;4.9&quot; &quot;4.7&quot; &quot;4.6&quot; &quot;5&quot; &quot;5.4&quot; &quot;4.4&quot; &quot;4.8&quot; &quot;4.3&quot; &quot;5.8&quot; &quot;5.7&quot; &quot;5.2&quot; ## [13] &quot;5.5&quot; &quot;4.5&quot; &quot;n/a&quot; &quot;5.3&quot; &quot;7&quot; &quot;6.4&quot; &quot;6.9&quot; &quot;6.5&quot; &quot;6.3&quot; &quot;6.6&quot; &quot;5.9&quot; &quot;6&quot; ## [25] &quot;6.1&quot; &quot;5.6&quot; &quot;6.7&quot; &quot;6.2&quot; &quot;6.8&quot; &quot;7.1&quot; &quot;7.6&quot; &quot; &quot; &quot;7.2&quot; &quot;7.7&quot; &quot;7.4&quot; &quot;7.9&quot; Looking at this, we can find the culprit for why these values are treated as a string. There are some entries that are simply a blank space \"\" and there are missing data encoded as \"n/a\" in this dataset. The read.csv function has a nice way to deal with this, specifically the argument na.strings that takes options for elements that might be present in the loaded data that should be treated as an NA (worksheet tasks 3.1.2A through worksheet task 3.1.2D). irisz &lt;- read.csv(&quot;data/iris.csv&quot;, na.strings = c(&quot;&quot;, &quot;n/a&quot;), stringsAsFactors = TRUE) str(irisz) ## &#39;data.frame&#39;: 150 obs. of 6 variables: ## $ Sepal.Length: num 5.1 4.9 4.7 4.6 5 5.4 4.6 5 4.4 4.9 ... ## $ Sepal.Width : num 3.5 3 3.2 3.1 3.6 NA 3.4 3.4 2.9 3.1 ... ## $ Petal.Length: num 1.4 1.4 1.3 1.5 1.4 1.7 1.4 1.5 1.4 1.5 ... ## $ Petal.Width : num 0.2 0.2 0.2 0.2 0.2 0.4 0.3 0.2 0.2 0.1 ... ## $ Species : Factor w/ 3 levels &quot;setosa&quot;,&quot;versicolor&quot;,..: 1 1 1 1 1 1 1 NA 1 1 ... ## $ Location : Factor w/ 6 levels &quot;Canada&quot;,&quot;China&quot;,..: 4 2 4 2 2 1 2 2 5 3 ... summary(irisz) ## Sepal.Length Sepal.Width Petal.Length Petal.Width ## Min. :4.300 Min. :0.350 Min. :1.000 Min. : 0.100 ## 1st Qu.:5.100 1st Qu.:2.800 1st Qu.:1.550 1st Qu.: 0.300 ## Median :5.800 Median :3.000 Median :4.300 Median : 1.300 ## Mean :5.839 Mean :3.028 Mean :3.752 Mean : 1.331 ## 3rd Qu.:6.400 3rd Qu.:3.300 3rd Qu.:5.100 3rd Qu.: 1.800 ## Max. :7.900 Max. :4.400 Max. :6.900 Max. :23.000 ## NA&#39;s :2 NA&#39;s :3 NA&#39;s :3 NA&#39;s :2 ## Species Location ## setosa :49 Canada:35 ## versicolor:50 China : 6 ## virginica :49 Japan :11 ## NA&#39;s : 2 Korea :10 ## Russia:11 ## USA :63 ## NA&#39;s :14 Now we can see that the first columns are being properly treated as numeric vectors. Our next step would be to double-check our data for anything that looks anamolous. Let’s first take a look at the distributions of our numerical data using the base R hist function, which plots a simple histogram (worksheet task 3.1.2E). hist(irisz$Sepal.Length) hist(irisz$Sepal.Width) From these figures, we can spot that there is one very low value for sepal width that we may want to investigate. hist(irisz$Petal.Length) hist(irisz$Petal.Width) From these figures, we see there is a very high value for petal width that we may want to investigate. It is possible that these values somehow got changed or corrupted during the recording process. We can filter out these anomalous pieces of data using conditional logic that we learned about when indexing and subsetting into a vector. Let’s start by filtering out rows that have a petal width more than 5. dim(irisz) ## [1] 150 6 irisz &lt;- irisz[irisz$Petal.Width &lt; 5, ] dim(irisz) ## [1] 149 6 Note that we used the statement irisz$Petal.Width &lt; 5 that evaluates as TRUE or FALSE for every row. We then indexed into the irisz dataframe using brackets [] and selected all rows that evaluate as TRUE. This led to one row being removed. We can repeat this with the anamolous value for sepal width (worksheet task 3.1.2F). dim(irisz) ## [1] 149 6 irisz &lt;- irisz[irisz$Sepal.Width &gt; 1, ] dim(irisz) ## [1] 148 6 Now that we have removed observations that looked strange by eye inspection, we may have only some final pre-processing steps to do. Depending on what downstream analysis we want to conduct, we may want to standardize or normalize our numeric values. R comes with an existing scale function that we can use to standardize between values with different ranges and orders of magnitude. ?scale The scale function will by default scale and center a numeric vector, but you can choose to do only one of these steps using argument settings. x &lt;- 1:10 scale(x) ## [,1] ## [1,] -1.4863011 ## [2,] -1.1560120 ## [3,] -0.8257228 ## [4,] -0.4954337 ## [5,] -0.1651446 ## [6,] 0.1651446 ## [7,] 0.4954337 ## [8,] 0.8257228 ## [9,] 1.1560120 ## [10,] 1.4863011 ## attr(,&quot;scaled:center&quot;) ## [1] 5.5 ## attr(,&quot;scaled:scale&quot;) ## [1] 3.02765 scale(x, center = FALSE) ## [,1] ## [1,] 0.1528942 ## [2,] 0.3057883 ## [3,] 0.4586825 ## [4,] 0.6115766 ## [5,] 0.7644708 ## [6,] 0.9173649 ## [7,] 1.0702591 ## [8,] 1.2231533 ## [9,] 1.3760474 ## [10,] 1.5289416 ## attr(,&quot;scaled:scale&quot;) ## [1] 6.540472 scale(x, scale = FALSE) ## [,1] ## [1,] -4.5 ## [2,] -3.5 ## [3,] -2.5 ## [4,] -1.5 ## [5,] -0.5 ## [6,] 0.5 ## [7,] 1.5 ## [8,] 2.5 ## [9,] 3.5 ## [10,] 4.5 ## attr(,&quot;scaled:center&quot;) ## [1] 5.5 We can scale the columns in our dataset and create a more standardized range of values for each column. # before scaling hist(irisz$Sepal.Length) hist(irisz$Petal.Width) # after scaling hist(scale(irisz$Sepal.Length)) hist(scale(irisz$Petal.Width)) The shapes of our distributions for these measurements is not fundamentally different, but if you look along the x-axis, you will notice that the values (especially the highest and lowest) are more similar between these two columns after they are both scaled. You may also consider rescaling values or normalizing them to all be between 0 and 1. There is a function for this called rescale that comes with the scales library. library(scales) rescale(x) ## [1] 0.0000000 0.1111111 0.2222222 0.3333333 0.4444444 0.5555556 0.6666667 ## [8] 0.7777778 0.8888889 1.0000000 "],["tidyverse-12-functions-pipes.html", "2.2 Tidyverse 1/2: functions &amp; pipes", " 2.2 Tidyverse 1/2: functions &amp; pipes Goal: Understand the tidyverse framework of pipes and functions, as well as how to apply it to tidy data frame cleaning and manipulation. 2.2.1 Welcome to the tidyverse! Tidyverse is a suite of R packages that are very useful for data science when we need to manipulate data frames and visualize data. The most important packages in tidyverse for our workshop will be dplyr and ggplot2. It builds on the base R functions and data types we’ve studied so far. It just provides a different design framework for working with data in R. The main two features of tidyverse are A new suite of very useful, intuitive functions The pipe %&gt;% which will allow us to go send data from one function to the next without using intermediate steps or nesting functions together 2.2.1.1 Preview: new functions in tidyverse Let’s say we have a dataframe, df, which has the name and age (in years) of everyone in a class. We want to create a new variable of age in months. In base R, we would do it this way: df$age_months &lt;- df$age_years * 12 In tidyverse, we can use the function mutate() to create a new variable. As with most tidyverse functions, mutate() takes the dataset as the first argument and the operation as the second. Here the operation is to create the new variable. mutate(df, age_months = age_years * 12) As you will see with more complex examples, these functions can make dataset operations more readable, especially when strung together with a pipe. 2.2.1.2 Preview: the pipe in tidyverse Recall the head() function used in the previous section. If you have a data frame, df, instead of peaking at the first few lines with head(df), you can pipe it into the head() function like this: df %&gt;% head(). This might seem like more work than just calling the function around the data frame object, but when you need to run a bunch of functions on a data frame, it becomes much TIDIER! In the previous example, we could call the function using a pipe like so: df %&gt;% mutate(age_months = age_years * 12) Feeling motivated to accelerate data cleaning and analysis using the tidyverse? 2.2.2 Load tidyverse Load the tidyverse library library(tidyverse) If tidyverse is already installed, loading the library should not return an error message. It is ok if it says there are package conflicts - this happens when different packages that are loaded have functions with the same name (but probably do different things). Just take note of the conflicts. If you use those other packages/functions then you’ll have to specify which package the function you’re using comes from. For example if mutate() was also in another package, to use the tidyverse version I would write tidyr::mutate() instead of just mutate() If loading the library returns an error message saying it’s not installed, then go to the SSRP Pre-installation guide and follow the instructions for installing tidyverse 2.2.3 How to pipe %&gt;% The tidyverse pipe can be used with most base R functions and with all tidyverse-specific functions, which we’ll learn more about soon. Although we’ll focus on using tidyverse pipe and functions on data frames, the pipe can also be used on vectors and even scalars. 2.2.3.0.1 Pipes with dataframes Let’s read in the iris dataset used in the previous section. Peak at it with head() to check that the columns look intact. # Load the dataset and peak at it with the head() function irisz &lt;- read.csv(&quot;data/iris.csv&quot;, na.strings = c(&quot;&quot;, &quot;n/a&quot;), stringsAsFactors = TRUE) head(irisz) ## Sepal.Length Sepal.Width Petal.Length Petal.Width Species Location ## 1 5.1 3.5 1.4 0.2 setosa Korea ## 2 4.9 3.0 1.4 0.2 setosa China ## 3 4.7 3.2 1.3 0.2 setosa Korea ## 4 4.6 3.1 1.5 0.2 setosa China ## 5 5.0 3.6 1.4 0.2 setosa China ## 6 5.4 NA 1.7 0.4 setosa Canada Now let’s try this using the pipe %&gt;%. irisz %&gt;% head() ## Sepal.Length Sepal.Width Petal.Length Petal.Width Species Location ## 1 5.1 3.5 1.4 0.2 setosa Korea ## 2 4.9 3.0 1.4 0.2 setosa China ## 3 4.7 3.2 1.3 0.2 setosa Korea ## 4 4.6 3.1 1.5 0.2 setosa China ## 5 5.0 3.6 1.4 0.2 setosa China ## 6 5.4 NA 1.7 0.4 setosa Canada Practice this with other functions we’ve used so far: summary(), unique(), str(), dim(), table(), and even hist()!. Do it with and without the pipe. Pretty simple right? Notice how the %&gt;% pipe is sending the output from before it to the function after it. This %&gt;% is similar to the | we use on the command line to send data from one operation to the next: sort data.txt | uniq -c (sort a data file and count the number of unique lines) Pipes can string together functions In the previous section, we learned how to read in a file with read.csv(), get unique values with unique(), and get the dimensions of a data frame with dim(). Let’s try these operations using the pipe instead of performing them as separate operations. irisz %&gt;% unique() %&gt;% dim() ## [1] 150 6 Notice how each operation is on one line so it’s easy to read the sequence of operations. Pipes with scalars To take the square root of a number x and round it to y digits, we would use the sqrt() and round() functions Here’s what this looks like with and without the pipe: x = 112 y = 2 # Two steps without the pipe z &lt;- sqrt(x) round(z, digits = y) ## [1] 10.58 # One step without the pipe round(sqrt(x), digits = y) ## [1] 10.58 # One step with the pipe sqrt(x) %&gt;% round(digits = y) ## [1] 10.58 Pipes with vectors We can apply the same piping method to vectors. Think about the vector that is the column Sepal.Length in the dataframe irisz which we have been using. We’ll use pipes to string together the following operations starting with the vector irisz$Sepal.Length: sort it, make the values numeric, and make a histogram. (These three functions were also introduced in the previous section.) # One step with the pipe irisz$Sepal.Length %&gt;% as.numeric() %&gt;% sort() %&gt;% hist() # One step without the pipe requires lots of parentheses hist(sort(as.numeric(irisz$Sepal.Length))) Hopefully by now you have a sense for how the pipe in tidyverse works and how it can work with functions we already know and can be applied to data frames, vectors, and scalars. Another big contribution tidyverse makes to the data science in R is the addition of new functions! 2.2.4 Essential tidyverse functions Let’s start with the essential functions of tidyverse. All of these are from the sub-package dplyr select(): subset columns filter(): subset rows on conditions arrange(): sort results mutate(): create new columns by using information from other columns group_by() and summarize(): create summary statistics on grouped data count(): count discrete values Reminder: if you ever want to know more about these functions, just use the help function - ? followed by the function of interest. For example: ?select 2.2.4.1 Selecting, deleting, reordering columns Selecting columns In base R, if we wanted to select certain columns (e.g. column names c1, c2, c6, c7) from a dataframe, df, we would do something like this: df[c(\"c1\", \"c2\", \"c6\", \"c7\")]. With tidyverse, we can use the select(c1, c2, c6, c7) function (from dplyr). Deleting columns To delete a column, just add a - before the column name inside of select(). Reordering columns Select can also be used to reorder columns. Just specify the order of columns in select(). Selecting and deselecting in the irisz dataset # Let&#39;s select just the location select(irisz, Location) # Remove the Location column select(irisz, -Location) Selecting multiple columns Let’s select both the Location and Species columns and see what unique combinations are present in the dataset unique(select(irisz, c(Location, Species))) ## Location Species ## 1 Korea setosa ## 2 China setosa ## 6 Canada setosa ## 8 China &lt;NA&gt; ## 9 Russia setosa ## 10 Japan setosa ## 46 &lt;NA&gt; setosa ## 51 &lt;NA&gt; versicolor ## 54 USA versicolor ## 61 Canada versicolor ## 101 USA virginica ## 107 &lt;NA&gt; virginica ## 143 &lt;NA&gt; &lt;NA&gt; # Note that the order returned is the order specified in select() # Originally Location came after Species, but now they are reordered. We can also do this in with the pipe. irisz %&gt;% select(Location, Species) %&gt;% unique() ## Location Species ## 1 Korea setosa ## 2 China setosa ## 6 Canada setosa ## 8 China &lt;NA&gt; ## 9 Russia setosa ## 10 Japan setosa ## 46 &lt;NA&gt; setosa ## 51 &lt;NA&gt; versicolor ## 54 USA versicolor ## 61 Canada versicolor ## 101 USA virginica ## 107 &lt;NA&gt; virginica ## 143 &lt;NA&gt; &lt;NA&gt; Note that select() subsets the dataframe to the specified columns. The output is a data frame (not a vector), even if only one column is selected What if we want to extract one column as a vector? In base R, we would use the $ operation to select a single column as a vector. With tidyverse, we can use the pull() function (from dplyr). Notice that pull() will only accept a single column name and will return a vector # Base R irisz$Location # With the pipe and tidyverse function pull() irisz %&gt;% pull(Location) 2.2.4.2 Filtering dataframes The basics of filtering data frames in tidyverse are the filter() function and boolean operators (&gt;, &lt;, ==) learned in section 2.4.3. Let’s try two examples: # Filter for Petal.Length greater than x x = 7 irisz %&gt;% filter(as.numeric(Sepal.Length) &gt; x) ## Sepal.Length Sepal.Width Petal.Length Petal.Width Species Location ## 1 7.1 3.0 5.9 2.1 virginica USA ## 2 7.6 3.0 6.6 2.1 virginica USA ## 3 7.2 3.6 6.1 2.5 virginica USA ## 4 7.7 3.8 6.7 2.2 virginica USA ## 5 7.7 2.6 6.9 23.0 virginica USA ## 6 7.7 2.8 6.7 2.0 virginica USA ## 7 7.2 3.2 6.0 1.8 virginica USA ## 8 7.2 3.0 5.8 1.6 virginica USA ## 9 7.4 2.8 6.1 1.9 virginica USA ## 10 7.9 3.8 6.4 2.0 virginica USA ## 11 7.7 3.0 6.1 2.3 virginica USA # Filter to get observations of setosa species from Korea irisz %&gt;% filter(Species == &quot;setosa&quot;, Location == &quot;Korea&quot;) ## Sepal.Length Sepal.Width Petal.Length Petal.Width Species Location ## 1 5.1 3.50 1.4 0.2 setosa Korea ## 2 4.7 3.20 1.3 0.2 setosa Korea ## 3 4.8 3.00 1.4 0.1 setosa Korea ## 4 5.4 3.90 1.3 0.4 setosa Korea ## 5 5.1 3.50 1.4 0.3 setosa Korea ## 6 5.1 3.80 1.5 0.3 setosa Korea ## 7 5.1 3.70 1.5 0.4 setosa Korea ## 8 5.5 0.35 1.3 0.2 setosa Korea ## 9 4.4 3.00 1.3 0.2 setosa Korea ## 10 NA NA 1.6 0.6 setosa Korea 2.2.4.3 Creating and renaming columns Creating columns in tidyverse is simple with the mutate() function. Let’s say we wanted to create a new column for the irisz dataset which has the row number. In base R, we would do something like irisz$row_number &lt;- seq(1,nrow(irisz)). Here, seq() creates a sequence of numbers from 1 to the number of rows in irisz. In tidyverse we can do it using the mutate() and row_number() functions. So it doesn’t output the whole data frame, we’ll pipe it directly into head() irisz %&gt;% mutate(row_number = row_number()) %&gt;% head() ## Sepal.Length Sepal.Width Petal.Length Petal.Width Species Location row_number ## 1 5.1 3.5 1.4 0.2 setosa Korea 1 ## 2 4.9 3.0 1.4 0.2 setosa China 2 ## 3 4.7 3.2 1.3 0.2 setosa Korea 3 ## 4 4.6 3.1 1.5 0.2 setosa China 4 ## 5 5.0 3.6 1.4 0.2 setosa China 5 ## 6 5.4 NA 1.7 0.4 setosa Canada 6 See the row number column? Mutate can also be used to update a column without creating a new one. # Change sepal length to be 10x what it currently is irisz %&gt;% mutate(Sepal.Length = 10*Sepal.Length ) %&gt;% head() ## Sepal.Length Sepal.Width Petal.Length Petal.Width Species Location ## 1 51 3.5 1.4 0.2 setosa Korea ## 2 49 3.0 1.4 0.2 setosa China ## 3 47 3.2 1.3 0.2 setosa Korea ## 4 46 3.1 1.5 0.2 setosa China ## 5 50 3.6 1.4 0.2 setosa China ## 6 54 NA 1.7 0.4 setosa Canada What if all the setosa measurements were actually supposed to be the virginica species? Let’s use the ifelse() function which accepts a conditional test, value if true, value if false. irisz %&gt;% # If Species is setosa, change it to virginica, otherwise use the value in Species mutate(Species = ifelse(Species == &quot;setosa&quot;, &quot;virginica&quot;, Species )) %&gt;% head() ## Sepal.Length Sepal.Width Petal.Length Petal.Width Species Location ## 1 5.1 3.5 1.4 0.2 virginica Korea ## 2 4.9 3.0 1.4 0.2 virginica China ## 3 4.7 3.2 1.3 0.2 virginica Korea ## 4 4.6 3.1 1.5 0.2 virginica China ## 5 5.0 3.6 1.4 0.2 virginica China ## 6 5.4 NA 1.7 0.4 virginica Canada Renaming columns Sometimes, we just need to rename an existing column. Use rename() to specify the new and old column names, rename(new_name = old_name). Let’s rename the location column to be “Country”. Let’s just pipe it directly into colnames() which will give us the column names of the data frame. irisz %&gt;% rename(Country = Location) %&gt;% colnames() ## [1] &quot;Sepal.Length&quot; &quot;Sepal.Width&quot; &quot;Petal.Length&quot; &quot;Petal.Width&quot; &quot;Species&quot; ## [6] &quot;Country&quot; Want to change row names into a column or a column into a row name? Check out rownames_to_column() and column_to_rownames(). 2.2.4.4 Summarizing data Summarizing data is very important in data analysis. It allows you to get a big picture overview of a new dataset: type, range of values, etc. These are also types of summaries to think about including when in the exploratory stages or a project: before running analyses on a dataset, create a summary of the dataset. Present this during your update meetings for your summer project and keep these summaries on hand in case you or others have questions about the dataset throughout the course of your project. For summarizing data in tidyverse, let’s explore the functions count() group_by() summarize() The function count() does what it sounds like - it counts the number of unique observations for the specified column(s) Count on one variable in the data frame irisz %&gt;% count(Location) ## Location n ## 1 Canada 35 ## 2 China 6 ## 3 Japan 11 ## 4 Korea 10 ## 5 Russia 11 ## 6 USA 63 ## 7 &lt;NA&gt; 14 Count on more than one variable in the data frame # Count the number of observations for each combination # of location and species irisz %&gt;% count(Location, Species) ## Location Species n ## 1 Canada setosa 11 ## 2 Canada versicolor 24 ## 3 China setosa 5 ## 4 China &lt;NA&gt; 1 ## 5 Japan setosa 11 ## 6 Korea setosa 10 ## 7 Russia setosa 11 ## 8 USA versicolor 20 ## 9 USA virginica 43 ## 10 &lt;NA&gt; setosa 1 ## 11 &lt;NA&gt; versicolor 6 ## 12 &lt;NA&gt; virginica 6 ## 13 &lt;NA&gt; &lt;NA&gt; 1 Can you think of another (perhaps longer) way to count using other functions, such as select(), unique(), n(), pull(), or table()? Summarizing WITHIN groups What if we want to do more than just count the number of occurrences for a given group? group_by takes a variable and groups the data by that variable. Anything that happens after that is only calculated within the group variable. To turn off grouping, use ungroup(). It’s always good practice to ungroup() after you’re done grouping. Although it doesn’t matter if you won’t be doing any more operations on the dataset. # Here&#39;s an example that produces a similar result # as in the count() example above irisz %&gt;% group_by(Location) %&gt;% summarize(n = n()) %&gt;% ungroup() ## # A tibble: 7 × 2 ## Location n ## &lt;fct&gt; &lt;int&gt; ## 1 Canada 35 ## 2 China 6 ## 3 Japan 11 ## 4 Korea 10 ## 5 Russia 11 ## 6 USA 63 ## 7 &lt;NA&gt; 14 Try taking away the ungroup() and see if it changes the result. Let’s build on this. We also want to calculate the mean() and sd() for a new variable called size for each location. Size is going to be length * width of the sepal. Our process is: Create a new column for size using mutate() Group the data by Location using group_by() Summarize the mean and standard deviation of the group using summarize() # Since mean() and sd() require numeric inputs, # We&#39;ll also tell mean() and sd() to ignore NAs in the calculation irisz %&gt;% mutate(size = as.numeric(Sepal.Length) * as.numeric(Sepal.Width)) %&gt;% group_by(Location) %&gt;% summarize(n = n(), mean_size = mean(Petal.Length, na.rm = T), mean_sd = sd(Petal.Length, na.rm = T)) %&gt;% ungroup() ## # A tibble: 7 × 4 ## Location n mean_size mean_sd ## &lt;fct&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Canada 35 3.39 1.30 ## 2 China 6 1.52 0.194 ## 3 Japan 11 1.43 0.205 ## 4 Korea 10 1.4 0.105 ## 5 Russia 11 1.48 0.218 ## 6 USA 63 5.21 0.826 ## 7 &lt;NA&gt; 14 4.51 0.990 For a further challenge, let’s clean up this table. Arrange the data so the countries with largest number of observations are at the top. Also, remove observations with no location. 2.2.4.5 Recap: Tidyverse vs Base R To review, tidyverse is just another way to code in R with new functions and a framework based on the pipe %&gt;%. You can always use a combination of both base R functions and tidyverse functions. There are some great resources on tidyverse functions. Here are a couple: Beginner’s tidyverse cheat sheet. The dplyr section includes the functions we focused on today. The ggplot2 section is what we’ll go over in the next section. Introduction to tidyverse includes a more comprehensive overview of tidyverse, from reading in data to applying essential functions (what we did today) to more advanced topics such as reshaping data frames. "],["tidyverse-22-reshaping-data.html", "2.3 Tidyverse 2/2: reshaping data", " 2.3 Tidyverse 2/2: reshaping data Goal: Understand when reshaping a dataframe is necessary and how to do it using tidyverse functions gather() and spread() 2.3.1 Why tidy data So far we’ve used two datasets: irisz and msleep. Even though there is some missing data in these datasets, for the most part, they are quite neat. Notice how each line represents a single observation and each column represents a single variable. There is no redundancy in the rows or columns. Sometimes datasets will have columns with columns that mean the same thing but apply to only certain subsets of the data. Columns like this can be collapsed. On the other hand, sometimes data will be recorded together in the same column, when they are in fact two different types of data. Imagine if conservation status in the msleep dataset were appended to the name. If we wanted to perform an analysis based on conservation status, we would need to tidy that up by making sure it’s in a separate columns. 2.3.2 How to tidy data The reality is that datasets don’t always come in neat format. Before and during data analysis, we will often need to “tidy” datasets. tidyverse has 4 important functions for tidying data: gather(): change data from a wide to long format - i.e. reduce extra columns spread(): change data from a long to wide format - i.e. expand a column into multiple columns separate(): spearate a column into 2 or more columns based on a pattern or delimeter unite(): unite multiple columns into 1 In lieu of re-inventing the wheel, here’s an excellent explanation and illustration of the concept of tidy data and how to apply these functions to make data tidy. Read this excerpt from Data Science in R by Garret Grolemand https://garrettgman.github.io/tidying/ For the asynchronous portion, let’s apply these principles and functions to a new dataset. I’ll set up a few example datasets based on the original. Answer the questions starting from the appropriate example dataset (e.g. df_1, df_2, df_3). 2.3.3 Data exploration with tidyverse The dataset: New York Times COVID-19 cases Even though we’re just using this dataset to learn about tidyverse, it’s always important to read about the dataset. Downloading data can be difficult and we don’t want to dive into that if the data is not what we want. So always read about it first! Read the introduction on Github. Based on the description of the dataset, try to answer these questions: (no need to look at the actual data, the description should be sufficient) Why was this data collected? When was the data last updated? What kind of data is it? Is it genetic data, imaging data, text data? Is it descriptive, numeric, categorical? How was the data collected? One or many sources? Are there imperfections in the data? Limitations? E.g. missing data, different data meanings? We’ll read in the data straight from github. This is a lot of data (every county in the U.S.) so to make it more manageable downstream, we’ll filter the dataset to only include: A few states: California, Washington, Illinois, Nebraska, and Florida. Feel free to add your own state! The last total count of cases and deaths ## ── Attaching packages ─────────────────────────────────────── tidyverse 1.3.1 ── ## ✔ ggplot2 3.3.5 ✔ purrr 0.3.4 ## ✔ tibble 3.1.6 ✔ dplyr 1.0.8 ## ✔ tidyr 1.2.0 ✔ stringr 1.4.0 ## ✔ readr 2.1.2 ✔ forcats 0.5.1 ## ── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ── ## ✖ dplyr::filter() masks stats::filter() ## ✖ dplyr::lag() masks stats::lag() data_url = &quot;https://raw.githubusercontent.com/nytimes/covid-19-data/master/us-counties.csv&quot; favorite_states = c(&quot;California&quot;, &quot;Washington&quot;, &quot;Illinois&quot;, &quot;Nebraska&quot;, &quot;Florida&quot;) df &lt;- read.csv(data_url, header = T) %&gt;% filter(state %in% favorite_states) %&gt;% mutate(date = as.Date(date,format=&quot;%Y-%m-%d&quot;)) %&gt;% filter(date == max(date)) Test your knowledge! Use functions we learned in the last section to answer these questions: What is the last date for recorded cases? Is it the same for all states/counties? Are all of the states listed in favorite_states in the dataset? How many counties are there for each of the states? 2.3.3.1 Example datasets Use the following three datasets to complete the coding exercises in today’s worksheet. The datasets are based on the New York Times Data, which is loaded above. Example Dataset #1:df_1 Three columns: county, cases, deaths df_1 &lt;- df %&gt;% tidyr::gather(key = &quot;metric&quot;, value = &quot;count&quot;, c(cases, deaths)) %&gt;% arrange(state,county) head(df_1) ## date county state fips metric count ## 1 2022-05-13 Alameda California 6001 cases 284054 ## 2 2022-05-13 Alameda California 6001 deaths 1868 ## 3 2022-05-13 Alpine California 6003 cases 126 ## 4 2022-05-13 Alpine California 6003 deaths 0 ## 5 2022-05-13 Amador California 6005 cases 9242 ## 6 2022-05-13 Amador California 6005 deaths 86 Example Dataset #2:df_2 Five columns: date, fips, metric corresponding to count, count, location (county, state) df_2 &lt;- df %&gt;% tidyr::gather(key = &quot;metric&quot;, value = &quot;count&quot;, c(cases, deaths)) %&gt;% mutate(county_state_fips = paste0(county,&quot;,&quot;, state, &quot;,&quot;, fips)) %&gt;% select(-county,-state) head(df_2) ## date fips metric count county_state_fips ## 1 2022-05-13 6001 cases 284054 Alameda,California,6001 ## 2 2022-05-13 6003 cases 126 Alpine,California,6003 ## 3 2022-05-13 6005 cases 9242 Amador,California,6005 ## 4 2022-05-13 6007 cases 40181 Butte,California,6007 ## 5 2022-05-13 6009 cases 7754 Calaveras,California,6009 ## 6 2022-05-13 6011 cases 4549 Colusa,California,6011 Example Dataset #3:df_3 A column with the county name, followed by columns for each state with the number of cases for that state/county pair. df_3 &lt;- df %&gt;% select(county,state,cases) %&gt;% tidyr::spread(key = &quot;state&quot;, value = &quot;cases&quot;) head(df_3) ## county California Florida Illinois Nebraska Washington ## 1 Adams NA NA 22525 6541 4926 ## 2 Alachua NA 69008 NA NA NA ## 3 Alameda 284054 NA NA NA NA ## 4 Alexander NA NA 1288 NA NA ## 5 Alpine 126 NA NA NA NA ## 6 Amador 9242 NA NA NA NA Use these three datasets to complete the coding exercises in today’s worksheet "],["making-plots-with-ggplot2.html", "2.4 Making plots with ggplot2", " 2.4 Making plots with ggplot2 We will primarily be working in ggplot2 as it has the greatest degree of customization for visualization and offers many additional features over the basic plotting in R. library(ggplot2) library(tidyverse) 2.4.1 Getting started with a ggplot Most ggplot calls to create a figure take the following form (you can read more using help(ggplot)): ggplot(data = &lt;DATA&gt;, mapping = aes(&lt;MAPPINGS&gt;)) + &lt;GEOM_FUNCTION&gt;() We will practice using our mammalian sleep dataset. You can look up more info about this dataset using the help function and the dataset name, msleep. head(msleep) ## # A tibble: 6 × 11 ## name genus vore order conservation sleep_total sleep_rem sleep_cycle awake ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Cheetah Acin… carni Carn… lc 12.1 NA NA 11.9 ## 2 Owl mo… Aotus omni Prim… &lt;NA&gt; 17 1.8 NA 7 ## 3 Mounta… Aplo… herbi Rode… nt 14.4 2.4 NA 9.6 ## 4 Greate… Blar… omni Sori… lc 14.9 2.3 0.133 9.1 ## 5 Cow Bos herbi Arti… domesticated 4 0.7 0.667 20 ## 6 Three-… Brad… herbi Pilo… &lt;NA&gt; 14.4 2.2 0.767 9.6 ## # … with 2 more variables: brainwt &lt;dbl&gt;, bodywt &lt;dbl&gt; You will first use the ggplot() function and bind the plot to a specific data frame using the data argument. ggplot(data = msleep) You will next need to define a mapping (using the aesthetic or aes function), by selecting the variables to be plotted and specifying how to present them in the graph, e.g. as x/y positions or characteristics such as size, shape, color, etc. ggplot(data = msleep, aes(x = brainwt, y = sleep_rem)) You can then add “geoms” or graphical representations of the data in the plot (points, lines, bars). ggplot2 offers many different geoms including: geom_point() for scatter plots, dot plots, etc. geom_boxplot() for, well, boxplots! geom_line() for trend lines, time series, etc. To add a geom to the plot use the + operator. To plot using two continuous variables, we will use geom_point() first. To save your work-in-progress, you can assign the plot to a variable. First we establish our coordinate system. my.plot &lt;- ggplot(data = msleep, aes(x = brainwt, y = sleep_rem)) We can now draw the plot as a scatterplot with points to represent each mammal’s measurements from the msleep dataset (worksheet task 3.4.1A). my.plot + geom_point() ## Warning: Removed 35 rows containing missing values (geom_point). You might notice that all of the points are squished against the y-axis since many of the mammals in this dataset have low brain weights. summary(msleep$brainwt) ## Min. 1st Qu. Median Mean 3rd Qu. Max. NA&#39;s ## 0.00014 0.00290 0.01240 0.28158 0.12550 5.71200 27 As you can see with the summary function, the minimum and median values are very low, but there are a few mammals with high brainwt as you can see by the much larger maximum value in this vector. To make more useful plots, we can transform this value using log-scaling. While we will have to note that the new values do not exactly match the real-world measurements anymore, patterns we see (i.e. something that correlates with higher brain weights) will still hold true. msleep2 &lt;- msleep %&gt;% mutate(brainwt_log = log(brainwt)) ggplot(msleep2, aes(x = brainwt_log, y = sleep_rem)) + geom_point() ## Warning: Removed 35 rows containing missing values (geom_point). Here we use the mutate function to make a new variable called brainwt_log in our dataset (technically a new dataset copy that we have saved as msleep2). Plotting this variable as our x variable (i.e. independent variable), makes it easier to look for patterns (worksheet tasks 3.4.1B and 3.4.1C). 2.4.2 Changing plot aesthetics We can modify the appearance of the plot in two ways: either uniformly changing the appearance or having the appearance vary depending on information present in our data. Let’s explore how to modify our plots uniformly. We can change aspects of the points we plot such as transparency (“alpha”) and color by supplying them as arguments in the geom_point function. ggplot(data = msleep2, aes(x = brainwt_log, y = sleep_rem)) + geom_point(alpha = 0.5, color = &quot;blue&quot;) ## Warning: Removed 35 rows containing missing values (geom_point). Here we have made the plots semi-transparent and colored blue. You can try varying these values (e.g. change blue to a different color). You can also supply other arguments such as shape to use something other than a dot (worksheet task 3.4.2A). However, it is also possible to scale the color of the points by some variable present in the data. This approach means we can create a scatterplot that conveys more than just two variables’ worth of information (x-axis and y-axis) by having the color reflect a third variable. To do this, we specify the color inside the aesthetic mapping aes within the initial ggplot function. Same as how we told R to use a specific column by name for x or y coordinates, we specify which column to use for color (worksheet task 3.4.2B). ggplot(msleep2, aes(x = brainwt_log, y = sleep_rem, color = vore)) + geom_point() ## Warning: Removed 35 rows containing missing values (geom_point). This plot conveys not only the relationship between brainwt_log and sleep_rem, but each plot representing a different mammal now conveys what the feeding behavior of that mammal is. When generating visualizations, it is important to annotate the figure with meaningful labels on the axes to make them accessible for the viewer. For that, we can use the labs function (worksheet task 3.4.2C). ggplot(msleep2, aes(x = brainwt_log, y = sleep_rem, color = vore)) + geom_point() + labs(x = &quot;Brain Weight (log)&quot;, y = &quot;Duration of REM Sleep&quot;, color = &quot;Feeding Behavior&quot;) ## Warning: Removed 35 rows containing missing values (geom_point). 2.4.3 Exploring simple plots Let’s consider how we make other plots besides a scatterplot. Scatterplots are a great way to look at two quantitative (numerical) values at the same time to observe patterns (i.e. correlations) between the variables or to identify interesting outliers. However, other plots may be more useful to look at differing numbers of variables (i.e. one quantitative variable) or different types of variables (i.e. qualitative or categorical data). Here, we discuss two types of single variable plots that look at either a quantitative variable (histogram) or a categorical variable (barplot). We can create histograms in ggplot2 that are more aesthetically pleasing than the default hist function. This shows the distribution of one quantitative variable (worksheet tasks 3.4.3A and 3.4.3B). ggplot(msleep, aes(x = sleep_total)) + geom_histogram(bins = 10) We can look at how many individuals in the dataset fall into each category, such as how many mammals have each kind of feeding behavior (worksheet task 3.4.3C). ggplot(data = msleep, aes(x = vore)) + geom_bar() As you can see where we map the aesthetic, we only tell the ggplot function to refer to a single column in our dataset for our x-axis. 2.4.4 Visualizing between groups Let’s return to iris dataset to explore how we can visualize differences between groups/categories. These groups are often represented in our data as a factor. We can look at how the distributions of Sepal.Length differ depending on which species each iris belongs to. One plot that can do this easily is the geom_boxplot function (worksheet task 3.4.4A). ggplot(data = iris, aes(x = Species, y = Sepal.Length)) + geom_boxplot() By adding a different parameter to fill in the aes we define throgh the ggplot function, we can separate out histograms according to different groupings. Here, we use Species to determine the color of the fill. ggplot(iris, aes(x = Sepal.Length, fill = Species)) + geom_histogram(bins = 10) While we can sort of see the trends on this plot, it may be helpful to separate out each histogram for each individual species. There is an easy way to do this in ggplot2 using facetting or the facet_wrap function. This function splits the figure into separate panel where the data has been filtered by the category (i.e. Species) (worksheet task 3.4.4B). ggplot(iris, aes(x = Sepal.Length, fill = Species)) + geom_histogram(bins = 10) + facet_wrap( ~ Species) This matches what we already saw in the boxplot, showing that there are different sepal lengths depending on which iris species we look at. We will explore in the next section how we know if these differences are significant. 2.4.5 Generating heatmaps (OPTIONAL) If you are already familiar with ggplot2 or progress quickly through that material, you may consider reading this subsection. Otherwise, feel free to skip ahead! Heatmaps are a useful way to show the values of multiple samples across many measurements. You can visualize a heatmap by thinking of your dataframe, this tabular data, if it had each cell colorcoded based on how high or low the value is. The base R heatmap function meets many needs while the ggplot2 equivalent (geom_tile) can be confusing so we will recommend that you not use ggplot2 for heatmaps. Let’s go back to msleep dataset to visualize trends between the different measurements taken on each mammal. We will first create a simplified dataset from msleep where we take the log value of both brainwt and bodywt (worksheet task 3.4.5A). temp.data &lt;- cbind(log(msleep$brainwt), log(msleep$bodywt)) head(temp.data) ## [,1] [,2] ## [1,] NA 3.9120230 ## [2,] -4.1669153 -0.7339692 ## [3,] NA 0.3001046 ## [4,] -8.1456296 -3.9633163 ## [5,] -0.8603831 6.3969297 ## [6,] NA 1.3480731 Next, we feed these numeric values into the heatmap function along with some arguments that specify settings for displaying the figure. We use labCol to specify how to label these columns and cexCol to control the text size of these labels. We set labRow to be the names of each species from the msleep dataset (worksheet task 3.4.5B). heatmap(temp.data, labRow = msleep$name, labCol = c(&quot;brainwt&quot;, &quot;bodywt&quot;), cexCol = 1) This first heatmap will look strange because it colors each box by its magnitude, but body weight of a mammal is always greater than its brain weight. We want to scale within each column so that the depth of the color reflects whether the mammal has a high brain weight or high body weight relative to the other mammals (worksheet task 3.4.5C). heatmap(temp.data, scale = &quot;col&quot;, labRow = msleep$name, labCol = c(&quot;brainwt&quot;, &quot;bodywt&quot;), cexCol = 1) Using the scale argument which we set to \"col\", now the color of the columns is more meaningful. For example, it makes sense that the measurements taken on an Asian elephant are much higher than those from a mole rat, so the color of those cells is deeper. Let’s add some more data to our heatmap visualization (worksheet task 3.4.5D and 3.4.5E). temp.data2 &lt;- cbind(temp.data, msleep$sleep_total, msleep$sleep_rem) heatmap(temp.data2, scale = &quot;col&quot;, labRow = msleep$name, labCol = c(&quot;brainwt&quot;, &quot;bodywt&quot;, &quot;total_sleep&quot;, &quot;rem_sleep&quot;), cexCol = 1) You may have noticed the weird diagrams along the top and left hand side of this heatmap. These strange line diagrams are trees that show how our samples cluster together. Mammals that have similar patterns of values across these four measurements are placed near each other in the diagram. "],["making-scientific-figures.html", "2.5 Making scientific figures", " 2.5 Making scientific figures Making plots can be a great way to develop an intuition for your dataset, though to derive and communicate scientific insights, we need to have an idea of the uncertainty in our interpretations. Uncertainty describes ideas such as: are the values between two groups different enough, that it is unlikely that the differences are due to chance? Is the correlation between these variables strong enough that one can predict the other, with some level of confidence? How statistically significant are the patterns we see? 2.5.1 Plotting error bars When we compare measurements taken from two samples (i.e. two groups), we might want to see if the two groups have very different values for that specific measurement. If we have multiple observations within each group, we can take a summary statistic such as the mean or median and plot those against each other. ggplot(msleep2, aes(x = vore, y = awake)) + geom_bar(stat = &quot;summary&quot;, fun = &quot;mean&quot;) For example, here we have asked our geom_bar function to plot a summary, specifically the mean of each group, instead of plotting identity which usually means the value as is. Looking at this figure, we can’t guess if the groups are significantly different without an idea of the uncertainty in our measurements through something like error bars. Here is the convention for plotting error bars in ggplot2, as you can see it is just another kind of geom that we can add to our plot: ggplot(data = &lt;SUMMARY DATA&gt;, mapping = aes(&lt;SUMMARY MAPPINGS&gt;) + geom_bar(stat = \"identity\") + geom_errorbar(aes(&lt;ERROR MAPPINGS&gt;)) This method is straightforward, but you need to have pre-calculated the summary statistic for each group and the amount of error (i.e. standard error) from your data. That “aggregated” dataframe becomes the data that you provide to ggplot, instead of the original dataset (worksheet task 3.5.1A). feeding.data &lt;- msleep2 %&gt;% group_by(vore) %&gt;% summarize(mean_se(awake)) feeding.data ## # A tibble: 5 × 4 ## vore y ymin ymax ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 carni 13.6 12.6 14.7 ## 2 herbi 14.5 13.6 15.4 ## 3 insecti 9.06 6.41 11.7 ## 4 omni 13.1 12.4 13.7 ## 5 &lt;NA&gt; 13.8 12.7 14.9 What does mean_se do? We can check. ?mean_se This function returns three values, y, ymin, and ymax which correspond to the mean, the mean minus one standard error, and the mean plus one standard error. The mean value will be the height of each bar on the barplot, while ymin corresponds to the bottom of the error bar and ymax to the top of the error bar. Let’s first plot the height of the bars using this new feeding.data dataset and mapping y to our new y column generated from the mean_se function. Here we create the same plot as before from this aggregated dataset, just showing the mean value in each group (worksheet task 3.5.1B). my.plot &lt;- ggplot(feeding.data, aes(x = vore, y = y)) + geom_bar(stat = &quot;identity&quot;) my.plot Now we add the error bars, mapping the ymin and ymax values to the arguments that happen to have the same name in the geom_errorbar function. We add in the width setting just for aesthetics (worksheet tasks 3.5.1C and 3.5.1D). my.plot &lt;- my.plot + geom_errorbar(aes(ymin = ymin, ymax = ymax), width = 0.2) my.plot Now it is more clear that insectivores have significantly less time awake compared to other kinds of mammals. 2.5.2 Showing trends in data Lots of these different figures summarize or aggregate the data. We may want to display the data with the individual points, but still show the overall trend across the data. Some good plots that do this are geom_density_2d which provides a contour plot. Let’s check out the trend in our msleep data between body weight and brain weight. We will create a new column where we take the log value of body weight, like we did with brain weight. msleep3 &lt;- msleep2 %&gt;% mutate(bodywt_log = log(bodywt)) ggplot(data = msleep3, mapping = aes(x = brainwt_log, y = bodywt_log)) + geom_density_2d() + geom_point() As you can see the density of points almost look like they fit a line. As brain weight increases then body weight increases, or vice versa. We can add a trendline to this plot with the geom_smooth function (worksheet tasks 3.5.2A and 3.5.2B). ggplot(msleep3, aes(x = brainwt_log, y = bodywt_log)) + geom_point(alpha = 0.5) + geom_smooth() ## `geom_smooth()` using method = &#39;loess&#39; and formula &#39;y ~ x&#39; We can also do this with trendlines that summarize only certain subsets of the data, such as those that belong to a specific category. We will flip back to our iris dataset to look at how sepal length compares to sepal width between different species of iris. ggplot(data = iris, mapping = aes(x = Sepal.Length, y = Sepal.Width)) + geom_point() + geom_smooth(aes(color = Species)) ## `geom_smooth()` using method = &#39;loess&#39; and formula &#39;y ~ x&#39; We create a seaprate trendline for each by specifiying the aes with color mapping to Species within the geom_smooth function. The above plot may make it hard to see the data that is contributing to each trend line. Using facet_wrap again, we can split the figure into separate panel where the data has been filtered by the category (i.e. species) (worksheet task 3.5.2C). ggplot(data = iris, mapping = aes(x = Sepal.Length, y = Sepal.Width)) + geom_point() + geom_smooth(aes(color = Species)) + facet_wrap( ~ Species) ## `geom_smooth()` using method = &#39;loess&#39; and formula &#39;y ~ x&#39; Now it is clear that there is a positive correlation (as one goes up, the other goes up) in each species of iris, but some species have a sharper upward trend than others. 2.5.3 Saving figures locally As you produce analysis in your research, you may want to create high-quality images of your figures to then use in presentations or publications. There are two easy ways to save images as an individual file on your computer (worksheet tasks 3.5.3A and 3.5.3B), The first method uses ggsave to save the most recent ggplot figure you generated (worksheet task 3.5.3C). ggplot(msleep2, aes(x = brainwt_log, y = sleep_rem)) + geom_point() + geom_smooth(method = &quot;lm&quot;) ggsave(&quot;plot.png&quot;, width = 5, height = 5) This function will save the figure just produced by this code wherever your directory is currently. You can check your current directory with getwd() and change with setwd(&lt;FOLDER NAME&gt;). You can also provide a precise file path in the new file name. ggsave(&quot;~/Downloads/plot.png&quot;, width = 5, height = 5) Here is an alternative method for saving your figures: pdf(&quot;plot.pdf&quot;) # creates the file # png() also works if you want a different file format ggplot(msleep2, aes(x = brainwt_log, y = sleep_rem)) + geom_point() + geom_smooth(method = &quot;lm&quot;) dev.off() # finishes editing the file Any changes to the figure that are contained between the initial creation of the figure (i.e. the pdf command) and the dev.off command will be included in the final saved image (worksheet task 3.5.3D). However, the figure is being printed directly to the file it is writing and won’t appear elsewhere (worksheet task 3.5.3E). "],["applying-basic-stats.html", "2.6 Applying basic stats", " 2.6 Applying basic stats Following up on our analysis using visualizations, we will review different types of statistical tests that we can perform on data. We will again revisit the mammalian sleep dataset msleep. head(msleep) ## # A tibble: 6 × 11 ## name genus vore order conservation sleep_total sleep_rem sleep_cycle awake ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Cheetah Acin… carni Carn… lc 12.1 NA NA 11.9 ## 2 Owl mo… Aotus omni Prim… &lt;NA&gt; 17 1.8 NA 7 ## 3 Mounta… Aplo… herbi Rode… nt 14.4 2.4 NA 9.6 ## 4 Greate… Blar… omni Sori… lc 14.9 2.3 0.133 9.1 ## 5 Cow Bos herbi Arti… domesticated 4 0.7 0.667 20 ## 6 Three-… Brad… herbi Pilo… &lt;NA&gt; 14.4 2.2 0.767 9.6 ## # … with 2 more variables: brainwt &lt;dbl&gt;, bodywt &lt;dbl&gt; 2.6.1 Describing quantitative variables There are several ways to describe quantitative measurements. We might first look at the range of values using the quantile function, which returns the min, max, and median values as well as the 25th and 75th percentiles. quantile(msleep$brainwt, na.rm = TRUE) ## 0% 25% 50% 75% 100% ## 0.00014 0.00290 0.01240 0.12550 5.71200 Note that we are using the setting na.rm = TRUE as an argument to these functions so that it ignores NA values in the data. You can also directly isolate some of these values using the functions min or max. We might describe the center of our data, that is the mean or the median values using those respective function names. median(msleep$brainwt, na.rm = TRUE) ## [1] 0.0124 mean(msleep$brainwt, na.rm = TRUE) ## [1] 0.2815814 We might also want to describe the spread in the data, such as the standard deviation or the interquartile range. sd(msleep$brainwt, na.rm = TRUE) ## [1] 0.9764137 IQR(msleep$brainwt, na.rm = TRUE) ## [1] 0.1226 Visualizations are a great way to see this information in one picture. We have learned to create a fancier histogram, but the hist function always works for quick viewing (worksheet tasks 3.6.1A and 3.6.1B). hist(msleep$brainwt) It is clear from this image that the data is skewed towards approaching 0 with a few high outliers. 2.6.2 Finding differences in data For the following statistical tests, we want to determine if measurements of a quantitative variable taken on a certain group of samples are different from similar measurements taken from a different group of samples. To know if these differences are statistically significant, we need to be aware of the uncertainty in our data. Even if we have collect many observations, there is generally noisiness and error in those measurements. Thus, we have uncertainty if the mean value we calculate for a group is in fact the true mean. We can calculate a confidence interval to describe our guess for the mean in our data. That is, we can identify the mean based on our samples, but also provide an upper or lower bound for where the mean might be (worksheet task 3.6.2A). t.test(msleep$brainwt) ## ## One Sample t-test ## ## data: msleep$brainwt ## t = 2.1581, df = 55, p-value = 0.03531 ## alternative hypothesis: true mean is not equal to 0 ## 95 percent confidence interval: ## 0.02009613 0.54306673 ## sample estimates: ## mean of x ## 0.2815814 We use this t.test function because we assume that our data is normally distributed (which is not necessarily always the case) and it has a small sample size. You can see that while the mean for brain weight from this popular is 0.28, the range is fairly large (between 0.2 and 0.54) because of how big the variability is in the data. If we want to determine if two populations have a difference in their mean values that is statistically significant, we can calculate the t-test between two sets of observations. Let’s look at whether the average time spent awake by mammals who are insectivores is significantly less than those who are not (worksheet task 3.6.2B). insectivores &lt;- msleep %&gt;% filter(vore == &quot;insecti&quot;) %&gt;% select(awake) other.mammals &lt;- msleep %&gt;% filter(vore != &quot;insecti&quot;) %&gt;% select(awake) head(insectivores) ## # A tibble: 5 × 1 ## awake ## &lt;dbl&gt; ## 1 4.3 ## 2 4.1 ## 3 5.9 ## 4 15.6 ## 5 15.4 head(other.mammals) ## # A tibble: 6 × 1 ## awake ## &lt;dbl&gt; ## 1 11.9 ## 2 7 ## 3 9.6 ## 4 9.1 ## 5 20 ## 6 9.6 At first, we use tidyverse functions to isolate only the mammals that match the feeding behavior and select only the relevant column, before feeding this data into the t.test function (worksheet tasks 3.6.2C and 3.6.2D). t.test(insectivores, other.mammals, alternative = &quot;less&quot;) ## ## Welch Two Sample t-test ## ## data: insectivores and other.mammals ## t = -1.7796, df = 4.3091, p-value = 0.0723 ## alternative hypothesis: true difference in means is less than 0 ## 95 percent confidence interval: ## -Inf 0.8336811 ## sample estimates: ## mean of x mean of y ## 9.06000 13.86056 These results show a p-value of 0.07, which is not the usual level of significance that most scientists accept (p-value &lt; 0.05). However, there are numerous feeding behaviors besides insectivore and we may be interested in comparing the multiple categories against each other. We can use ANOVA (analysis of variance) to see how our quantitative variable is affected by the different kinds of feeding behavior (worksheet tasks 3.6.2E and 3.6.2F). anova.msleep &lt;- aov(awake ~ vore, data = msleep) summary(anova.msleep) ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## vore 3 133.7 44.58 2.234 0.0916 . ## Residuals 72 1437.0 19.96 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## 7 observations deleted due to missingness The p-value of 0.09 suggests that there may not be a strongly significant difference in the average time spent awake between these groups. 2.6.3 Identifying correlations When we are examining the relationship between two quantitative variables, we might be interested in whether they correlate with one another. A positive correlation between x and y means that as x increases so does y. A negative correlation between x and y means that as x increases, y decreases. We can use the base R cor function to calculate the correlation between two numeric vectors. cor(msleep$brainwt, msleep$bodywt) ## [1] NA Notice that we get NA instead of an actual numeric value. Unfortunately there is not an easy setting to have the cor function ignore any NA values. Thus, we need to temporarily remove these values. Let’s first find them with the summary function. summary(msleep$bodywt) ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 0.005 0.174 1.670 166.136 41.750 6654.000 summary(msleep$brainwt) ## Min. 1st Qu. Median Mean 3rd Qu. Max. NA&#39;s ## 0.00014 0.00290 0.01240 0.28158 0.12550 5.71200 27 There are no NA values in the bodywt column, but there are 27 NA values in the brainwt column. Let’s remove those using a helpful function in tidyverse and save the more complete dataset as msleep2 (worksheet task 3.6.3A). msleep2 &lt;- msleep %&gt;% drop_na(brainwt) dim(msleep) ## [1] 83 11 dim(msleep2) ## [1] 56 11 You can see that we dropped 27 rows, corresponding to the mammals for whom brain weight was not measured or available. Now we can try to use the cor function. cor(msleep2$brainwt, msleep2$bodywt) ## [1] 0.9337822 We can see that the correlation is 0.93, a value very close to 1. This suggests that these variabilities are almost perfectly correlated with each other, which we saw when we plotted these two variables against each other in the prior section. There are multiple types of correlations. The cor function uses a Pearson correlation by default and can use different methods like Spearman. Let’s explore the difference between these correlations by looking at the relationship between the log value of brain weight and length of sleep cycle. msleep3 &lt;- msleep %&gt;% mutate(brainwt_log = log(brainwt)) %&gt;% drop_na(brainwt_log, sleep_cycle) ggplot(msleep3, aes(x = brainwt_log, y = sleep_cycle)) + geom_point() + geom_smooth(method = &quot;lm&quot;) ## `geom_smooth()` using formula &#39;y ~ x&#39; When we plot these variables against each other, including a fitted linear trend line, we can see that there is a relationship between these variables (worksheet task 3.6.3B). However, the points do not quite fit a straight line if we look at the edges and center of this fitted line. This will reflect in our calculation of two different correlations (worksheet task 3.6.3C). cor(msleep3$brainwt_log, msleep3$sleep_cycle, method = &quot;pearson&quot;) ## [1] 0.8331014 cor(msleep3$brainwt_log, msleep3$sleep_cycle, method = &quot;spearman&quot;) ## [1] 0.8726893 Pearson correlations are higher when the data fits to a straight line like a trend line. However, Spearman correlations do not require a single slope: as long as one value goes up and the other goes up, these values are considered highly correlated. 2.6.4 Producing linear models You may be interested in studying the relationship between one or more variables and some outcome that you care about. To determine if this set of one or more variables are strong predictors of an outcome (some quantitative variable), you can fit a linear model to this data using the lm function. Here we train a linear model to try to predict brain weight from body weight in the msleep dataset. Linear models are established with a formula with the format of outcome ~ predictor where we are trying to determine if the predictor is able to help us accurately estimate an outcome (worksheet task 3.6.4A). my.mod &lt;- lm(formula = brainwt ~ bodywt, data = msleep2) summary(my.mod) ## ## Call: ## lm(formula = brainwt ~ bodywt, data = msleep2) ## ## Residuals: ## Min 1Q Median 3Q Max ## -0.78804 -0.08422 -0.07634 -0.02839 2.06190 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 8.592e-02 4.821e-02 1.782 0.0804 . ## bodywt 9.639e-04 5.027e-05 19.176 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.3526 on 54 degrees of freedom ## Multiple R-squared: 0.8719, Adjusted R-squared: 0.8696 ## F-statistic: 367.7 on 1 and 54 DF, p-value: &lt; 2.2e-16 In the summary of the results of this modeling, we see that bodywt is a significant predictor of brainwt in the msleep dataset. We can provide more predictors into our formula in the format: outcome ~ predictor1 + predictor2 + ... (worksheet tasks 3.6.4B and 3.6.4C). my.mod2 &lt;- lm(formula = sleep_total ~ bodywt + brainwt, data = msleep2) summary(my.mod2) ## ## Call: ## lm(formula = sleep_total ~ bodywt + brainwt, data = msleep2) ## ## Residuals: ## Min 1Q Median 3Q Max ## -7.4531 -2.2517 -0.2619 2.0531 9.2283 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 10.6722485 0.5898643 18.093 &lt;2e-16 *** ## bodywt 0.0007953 0.0016703 0.476 0.636 ## brainwt -2.3518943 1.6180072 -1.454 0.152 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 4.193 on 53 degrees of freedom ## Multiple R-squared: 0.1337, Adjusted R-squared: 0.101 ## F-statistic: 4.088 on 2 and 53 DF, p-value: 0.02232 These results suggest that neither bodywt nor brainwt are significant predictors of sleep_total. That means that the value of either of those measurements does not give us information that helps us guess the total sleep duration for a mammal accurately. "],["appendix.html", "3 Appendix ", " 3 Appendix "],["some-links-and-some-extras.html", "3.1 Some links and some extras", " 3.1 Some links and some extras This is for collecting factoids, ideas, and links that may be useful as reference (if not inspiration) for your future R practice. 3.1.1 Ideas for figures An interactive page showing different types of figures A R graph gallery Claus Wilke’s cowplot package is a great way to make multi-panel figures with ggplot, so A B etc. 3.1.2 More links swirl is an excellent interactive tutorial package you can install and work through yourself for more practice, get started here A whole bunch of cheatsheets and example sheets of common commands and ways to do things, check under “Contributed Cheatsheets” for some great ones, or browse them all Claus Wilke’s dataviz bookdown R Programming for Data Science Some basic non-tidyverse common commands "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
